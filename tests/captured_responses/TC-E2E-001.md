## Summary
The recommended architecture is a batch EL pipeline with Cloud Run extraction, GCS landing, and BigQuery raw to curated loads, because a daily run with a 2-hour SLA does not require streaming complexity.

## Decision
Use a daily architecture: Salesforce API -> Cloud Run extractor -> GCS (Avro) -> BigQuery load job -> curated model. Enforce idempotent partition loads and produce both a data contract and runbook deliverable.

## Rationale
This architecture aligns with the SLA while minimizing operational overhead. It supports fail-loud behavior through explicit validation checks and predictable retries. The data contract defines schema and ownership boundaries; the runbook defines alerting, backfill, and incident response.

## Trade-offs
| Option | Pros | Cons | When to Use |
|--------|------|------|-------------|
| Batch EL architecture | Simpler operations, lower cost, easier backfill | Not sub-minute freshness | Daily/hourly reporting |
| Streaming architecture | Low-latency updates | Higher complexity and cost | True real-time use cases |

## Next Steps
1. Finalize source objects, destination tables, and daily load window.
2. Draft and approve the data contract with schema, SLAs, and ownership.
3. Implement the runbook with escalation, retries, and backfill procedure.
4. Add idempotent partition overwrite and row-count reconciliation checks.

## Template
Use `templates/data_contract.yaml` and `templates/runbook.md` for implementation artifacts.
